#!/usr/bin/env python3
"""
SEO Article Extractor - ç®€åŒ–ç‰ˆæœ¬
ä» urls.txt æ–‡ä»¶ä¸­è¯»å– URLï¼Œè‡ªåŠ¨æå–æ–‡ç« å†…å®¹
"""

import os
import sys
import requests
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
import statistics

class SEOArticleExtractor:
    def __init__(self, keyword):
        self.keyword = keyword
        self.output_dir = f"research_{keyword.replace(' ', '_')}"
        Path(self.output_dir).mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.word_counts = []
        self.log_file = f"{self.output_dir}/log.txt"
        self._log(f"\\n{'='*80}")
        self._log(f"SEO Article Extractor Started")
        self._log(f"{'='*80}")
        self._log(f"æ—¶é—´ï¼š{self.timestamp}")
        self._log(f"å…³é”®è¯ï¼š{self.keyword}")
    
    def _log(self, message):
        """è®°å½•æ“ä½œ"""
        print(message)
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(f"{message}\\n")
    
    def extract_from_urls(self):
        """ä» urls.txt ä¸­è¯»å– URL å¹¶æå–æ–‡ç« """
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 1ã€‘ä» URLs.txt æå–æ–‡ç« å†…å®¹")
        self._log(f"{'='*80}\\n")
        
        urls_file = "urls.txt"
        
        # æ£€æŸ¥ urls.txt æ˜¯å¦å­˜åœ¨
        if not Path(urls_file).exists():
            self._log(f"âŒ æ‰¾ä¸åˆ° {urls_file} æ–‡ä»¶")
            self._log(f"è¯·åœ¨å½“å‰ç›®å½•åˆ›å»º urls.txt æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ª URL")
            return False
        
        # è¯»å– URLs
        try:
            with open(urls_file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f.readlines() if line.strip()]
        except Exception as e:
            self._log(f"âŒ æ— æ³•è¯»å– urls.txtï¼š{e}")
            return False
        
        self._log(f"ğŸ“„ æ‰¾åˆ° {len(urls)} ä¸ª URLs\\n")
        
        # é€ä¸ªæå–æ–‡ç« 
        for i, url in enumerate(urls[:5], 1):  # æœ€å¤šå¤„ç†å‰ 5 ä¸ª
            self._log(f"ğŸ“¥ æ­£åœ¨æå–æ–‡ç«  {i}: {url}")
            
            try:
                # ä¸‹è½½ç½‘é¡µ
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10)
                response.encoding = 'utf-8'
                
                if response.status_code == 200:
                    # ä½¿ç”¨ BeautifulSoup è§£æ HTML
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                    for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'form']):
                        element.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text(separator='\\n')
                    
                    # æ¸…ç†æ–‡æœ¬
                    lines = [line.strip() for line in text.split('\\n') if line.strip()]
                    clean_text = '\\n'.join(lines)
                    
                    # ä¿å­˜æ–‡ç« 
                    if len(clean_text) > 500:
                        file_path = f"{self.output_dir}/article_{i}.txt"
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(clean_text)
                        
                        word_count = len(clean_text.split())
                        self._log(f"âœ… æˆåŠŸæå–ï¼š{word_count:,} å­—\\n")
                    else:
                        self._log(f"âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡\\n")
                else:
                    self._log(f"âš ï¸ HTTP {response.status_code}ï¼Œæ— æ³•è®¿é—®\\n")
                    
            except Exception as e:
                self._log(f"âš ï¸ é”™è¯¯ï¼š{str(e)}\\n")
        
        return True
    
    def analyze_word_count(self):
        """åˆ†æå­—æ•°"""
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 2ã€‘å­—æ•°åˆ†æ")
        self._log(f"{'='*80}\\n")
        
        articles = sorted(Path(self.output_dir).glob('article_*.txt'))
        
        if not articles:
            self._log("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« æ–‡ä»¶\\n")
            return 3000
        
        for i, path in enumerate(articles, 1):
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            word_count = len(content.split())
            self.word_counts.append(word_count)
            self._log(f"  ğŸ“„ æ–‡ç«  {i}: {word_count:,} å­—")
        
        if self.word_counts:
            avg = statistics.mean(self.word_counts)
            median = statistics.median(self.word_counts)
            recommended = int((avg + median) / 2)
            
            self._log(f"\\nâœ… æ¨èå­—æ•°: {recommended:,}\\n")
            return recommended
        
        return 3000
    
    def generate_outline(self):
        """ç”Ÿæˆå¤§çº²"""
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 3ã€‘ç”Ÿæˆæœ€ä¼˜å¤§çº²")
        self._log(f"{'='*80}\\n")
        
        outline = f"""# {self.keyword.title()}

## ç®€ä»‹
- å®šä¹‰æ¦‚å¿µ
- ä¸ºä»€ä¹ˆé‡è¦

## å¥½å¤„
- å¥½å¤„ 1
- å¥½å¤„ 2

## å·¥ä½œåŸç†
- æ¦‚å¿µ 1
- æ¦‚å¿µ 2

## æœ€ä½³å®è·µ
- å®è·µ 1
- å®è·µ 2

## å¸¸è§é”™è¯¯
- é”™è¯¯ 1
- é”™è¯¯ 2

## å·¥å…·
- å·¥å…· 1
- å·¥å…· 2

## å¸¸è§é—®é¢˜
- Q1: ...?
- Q2: ...?

## ç»“è®º
- æ€»ç»“
- ä¸‹ä¸€æ­¥
"""
        
        outline_path = f"{self.output_dir}/outline.md"
        with open(outline_path, 'w', encoding='utf-8') as f:
            f.write(outline)
        
        self._log("âœ… å¤§çº²å·²ç”Ÿæˆ\\n")
        self._log(outline)
        return outline
    
    def generate_writing_prompt(self, word_count, outline):
        """ç”Ÿæˆå†™ä½œæç¤º"""
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 4ã€‘ç”Ÿæˆ AI å†™ä½œæç¤º")
        self._log(f"{'='*80}\\n")
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ SEO å†…å®¹åˆ›ä½œè€…ã€‚æ ¹æ®ä»¥ä¸‹è¦æ±‚å†™ä¸€ç¯‡æ–‡ç« ã€‚

ã€è¦æ±‚ã€‘
- å…³é”®è¯ï¼š{self.keyword}
- å­—æ•°ï¼š{word_count:,} å­—
- æ ¼å¼ï¼šMarkdown

ã€å¤§çº²ã€‘
{outline}

ã€å…³é”®æŒ‡ä»¤ã€‘
1. é¿å… AI é£æ ¼
   - è¯´"æˆ‘å‘ç°..."è€Œä¸æ˜¯"ç ”ç©¶è¡¨æ˜..."
   - è¡¨è¾¾çœŸå®è§‚ç‚¹
   - é¿å…ï¼š"åœ¨å½“ä»Š...","ç»¼åˆæ¥çœ‹...","å€¼å¾—ä¸€æçš„æ˜¯..."

2. æ·»åŠ ä¸ªäººç»éªŒ
   - è‡³å°‘ 2-3 ä¸ªçœŸå®æ¡ˆä¾‹
   - åˆ†äº«å¤±è´¥ç»å†
   - ä½¿ç”¨å…·ä½“æ•°å­—

3. å˜åŒ–å¥å¼ç»“æ„
   - æ··åˆçŸ­å¥å’Œé•¿å¥
   - æ¯æ®µ 3-4 å¥
   - æ®µè½å¼€å¤´ç”¨ä¸»é¢˜å¥

4. åˆ›é€ å¯¹è¯æ„Ÿ
   - ä½¿ç”¨ä¿®è¾æ€§é—®é¢˜
   - è¡¨è¾¾å›°æƒ‘å’Œæ€è€ƒ
   - é‚€è¯·è¯»è€…æ€è€ƒ

ã€SEO ä¼˜åŒ–ã€‘
- å…³é”®è¯å¯†åº¦ï¼š1-2%
- åœ¨å‰ 100 å­—å‡ºç°ä¸»å…³é”®è¯
- åœ¨ H2/H3 ä¸­èå…¥é•¿å°¾è¯
- åŒ…å«åˆ—è¡¨å’Œè¡¨æ ¼
- åŒ…å« FAQ éƒ¨åˆ†

ç°åœ¨è¯·å†™å‡ºè¿™ç¯‡æ–‡ç« ã€‚å­—æ•° {int(word_count * 0.9)} - {int(word_count * 1.1)} ä¹‹é—´ã€‚"""
        
        prompt_path = f"{self.output_dir}/writing_prompt.txt"
        with open(prompt_path, 'w', encoding='utf-8') as f:
            f.write(prompt)
        
        self._log("âœ… å†™ä½œæç¤ºå·²ç”Ÿæˆ\\n")
        self._log("ã€ä¸‹ä¸€æ­¥ï¼šå¤åˆ¶æç¤ºå†…å®¹åˆ° Claudeã€‘\\n")
        self._log("="*80)
        self._log(prompt[:500] + "... [å†…å®¹ç»§ç»­] ...")
        self._log("="*80 + "\\n")
        
        return prompt
    
    def run_all(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        success = self.extract_from_urls()
        
        if not success:
            return
        
        word_count = self.analyze_word_count()
        outline = self.generate_outline()
        self.generate_writing_prompt(word_count, outline)
        
        self._log(f"\\n{'='*80}")
        self._log("âœ… æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
        self._log(f"{'='*80}")
        self._log(f"\\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨ï¼š{self.output_dir}/\\n")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        keyword = input("è¯·è¾“å…¥å…³é”®è¯ï¼š")
    else:
        keyword = " ".join(sys.argv[1:])
    
    extractor = SEOArticleExtractor(keyword)
    extractor.run_all()