#!/usr/bin/env python3

import os
import sys
import subprocess
from pathlib import Path
from datetime import datetime
from urllib.parse import quote
import statistics

class SEOArticleAutomation:
    def __init__(self, keyword):
        self.keyword = keyword
        self.output_dir = f"research_{keyword.replace(' ', '_')}"
        Path(self.output_dir).mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.word_counts = []
        self.log_file = f"{self.output_dir}/log.txt"
        self._log(f"\\n{'='*80}")
        self._log(f"SEO Article Automation Started: {self.timestamp}")
        self._log(f"å…³é”®è¯ï¼š{self.keyword}")
        self._log(f"{'='*80}")
    
    def _log(self, message):
        print(message)
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(f"{message}\\n")
    
    def step1_google_search_and_extract(self):
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 1ã€‘Google æœç´¢ + æ–‡ç« æå–")
        self._log(f"{'='*80}\\n")
        
        search_url = f"https://www.google.com/search?q={quote(self.keyword)}"
        self._log(f"ğŸ” æœç´¢ URL: {search_url}\\n")
        
        puppeteer_script = self._create_puppeteer_script(search_url)
        script_path = f"{self.output_dir}/google_search.js"
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(puppeteer_script)
        
        self._log(f"ğŸ“ è„šæœ¬å·²ç”Ÿæˆ: {script_path}")
        self._log(f"â³ æ­£åœ¨æ‰§è¡Œ... è¯·ç¨å€™ï¼ˆ30-60 ç§’ï¼‰\\n")
        
        try:
            result = subprocess.run(
                ['node', script_path],
                cwd=self.output_dir,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if result.returncode == 0:
                self._log("âœ… æœç´¢å’Œæå–å®Œæˆï¼\\n")
                return True
            else:
                self._log(f"âš ï¸ é”™è¯¯: {result.stderr}\\n")
                return False
        except Exception as e:
            self._log(f"âŒ é”™è¯¯: {e}\\n")
            return False
    
    def _create_puppeteer_script(self, search_url):
        return f"""
const puppeteer = require('puppeteer');
const fs = require('fs');

(async () => {{
    let browser;
    try {{
        console.log('ğŸš€ å¯åŠ¨æµè§ˆå™¨...');
        browser = await puppeteer.launch({{headless: true, args: ['--no-sandbox']}});
        
        const page = await browser.newPage();
        await page.setDefaultNavigationTimeout(30000);
        await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36');
        
        console.log('ğŸ” æ‰“å¼€ Google...');
        await page.goto('{search_url}', {{waitUntil: 'networkidle2', timeout: 30000}});
        await page.waitForSelector('div.g', {{ timeout: 10000 }});
        
        console.log('ğŸ“„ æå–æœç´¢ç»“æœ...');
        
        const results = await page.evaluate(() => {{
            const items = document.querySelectorAll('div.g');
            const topResults = [];
            
            for (let i = 0; i < items.length; i++) {{
                if (topResults.length >= 5) break;
                
                const linkElem = items[i].querySelector('a[href]');
                const titleElem = items[i].querySelector('h3');
                
                if (linkElem && titleElem) {{
                    const url = linkElem.href;
                    const title = titleElem.innerText;
                    
                    if (!url.includes('google.com') && 
                        !url.includes('youtube.com') &&
                        url.startsWith('http')) {{
                        topResults.push({{position: topResults.length + 1, title, url}});
                    }}
                }}
            }}
            return topResults;
        }});
        
        console.log('\\\\nâœ… æ‰¾åˆ° ' + results.length + ' ç¯‡æ–‡ç« ');
        
        for (let i = 0; i < results.length; i++) {{
            console.log('\\\\nğŸ“¥ æå–æ–‡ç«  ' + (i + 1));
            
            try {{
                const articlePage = await browser.newPage();
                await articlePage.setDefaultNavigationTimeout(20000);
                await articlePage.goto(results[i].url, {{waitUntil: 'networkidle2', timeout: 20000}});
                
                const articleText = await articlePage.evaluate(() => {{
                    document.querySelectorAll('script, style, nav, footer, aside').forEach(el => el.remove());
                    const article = document.querySelector('article') || document.querySelector('main') || document.body;
                    return article ? article.innerText : '';
                }});
                
                const cleanText = articleText.split('\\\\n').filter(line => line.trim()).join('\\\\n');
                
                if (cleanText.length > 500) {{
                    fs.writeFileSync('article_' + (i + 1) + '.txt', cleanText, 'utf-8');
                    console.log('âœ… ä¿å­˜æˆåŠŸ');
                }}
                
                await articlePage.close();
                await new Promise(resolve => setTimeout(resolve, 2000));
                
            }} catch (error) {{
                console.log('âš ï¸ æ— æ³•æå–');
            }}
        }}
        
        console.log('\\\\nâœ… å®Œæˆï¼');
        await browser.close();
        process.exit(0);
        
    }} catch (error) {{
        console.error('âŒ é”™è¯¯:', error.message);
        if (browser) await browser.close();
        process.exit(1);
    }}
}})();
"""
    
    def step2_word_count_analysis(self):
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 2ã€‘å­—æ•°åˆ†æ")
        self._log(f"{'='*80}\\n")
        
        articles = sorted(Path(self.output_dir).glob('article_*.txt'))
        
        if not articles:
            self._log("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« æ–‡ä»¶\\n")
            return 3000
        
        for i, path in enumerate(articles, 1):
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            word_count = len(content.split())
            self.word_counts.append(word_count)
            self._log(f"  ğŸ“„ æ–‡ç«  {i}: {word_count:,} å­—")
        
        if self.word_counts:
            avg = statistics.mean(self.word_counts)
            median = statistics.median(self.word_counts)
            recommended = int((avg + median) / 2)
            
            self._log(f"\\nâœ… æ¨èå­—æ•°: {recommended:,}\\n")
            return recommended
        
        return 3000
    
    def step3_outline_analysis(self):
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 3ã€‘ç”Ÿæˆæœ€ä¼˜å¤§çº²")
        self._log(f"{'='*80}\\n")
        
        outline = f"""# {self.keyword.title()}

## ç®€ä»‹
- å®šä¹‰æ¦‚å¿µ
- ä¸ºä»€ä¹ˆé‡è¦

## å¥½å¤„
- å¥½å¤„ 1
- å¥½å¤„ 2

## å·¥ä½œåŸç†
- æ¦‚å¿µ 1
- æ¦‚å¿µ 2

## æœ€ä½³å®è·µ
- å®è·µ 1
- å®è·µ 2

## å¸¸è§é”™è¯¯
- é”™è¯¯ 1
- é”™è¯¯ 2

## å·¥å…·
- å·¥å…· 1
- å·¥å…· 2

## å¸¸è§é—®é¢˜
- Q1: ...?
- Q2: ...?

## ç»“è®º
- æ€»ç»“
- ä¸‹ä¸€æ­¥
"""
        
        outline_path = f"{self.output_dir}/outline.md"
        with open(outline_path, 'w', encoding='utf-8') as f:
            f.write(outline)
        
        self._log("âœ… å¤§çº²å·²ç”Ÿæˆ\\n")
        self._log(outline)
        return outline
    
    def step4_ai_writing_prompt(self, word_count, outline):
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 4ã€‘ç”Ÿæˆ AI å†™ä½œæç¤º")
        self._log(f"{'='*80}\\n")
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ SEO å†…å®¹åˆ›ä½œè€…ã€‚æ ¹æ®ä»¥ä¸‹è¦æ±‚å†™ä¸€ç¯‡æ–‡ç« ã€‚

ã€è¦æ±‚ã€‘
- å…³é”®è¯ï¼š{self.keyword}
- å­—æ•°ï¼š{word_count:,} å­—
- æ ¼å¼ï¼šMarkdown

ã€å¤§çº²ã€‘
{outline}

ã€å…³é”®æŒ‡ä»¤ã€‘
1. é¿å… AI é£æ ¼
   - è¯´"æˆ‘å‘ç°..."è€Œä¸æ˜¯"ç ”ç©¶è¡¨æ˜..."
   - è¡¨è¾¾çœŸå®è§‚ç‚¹
   - é¿å…ï¼š"åœ¨å½“ä»Š...","ç»¼åˆæ¥çœ‹...","å€¼å¾—ä¸€æçš„æ˜¯..."

2. æ·»åŠ ä¸ªäººç»éªŒ
   - è‡³å°‘ 2-3 ä¸ªçœŸå®æ¡ˆä¾‹
   - åˆ†äº«å¤±è´¥ç»å†
   - ä½¿ç”¨å…·ä½“æ•°å­—

3. å˜åŒ–å¥å¼ç»“æ„
   - æ··åˆçŸ­å¥å’Œé•¿å¥
   - æ¯æ®µ 3-4 å¥
   - æ®µè½å¼€å¤´ç”¨ä¸»é¢˜å¥

4. åˆ›é€ å¯¹è¯æ„Ÿ
   - ä½¿ç”¨ä¿®è¾æ€§é—®é¢˜
   - è¡¨è¾¾å›°æƒ‘å’Œæ€è€ƒ
   - é‚€è¯·è¯»è€…æ€è€ƒ

ã€SEO ä¼˜åŒ–ã€‘
- å…³é”®è¯å¯†åº¦ï¼š1-2%
- åœ¨å‰ 100 å­—å‡ºç°ä¸»å…³é”®è¯
- åœ¨ H2/H3 ä¸­èå…¥é•¿å°¾è¯
- åŒ…å«åˆ—è¡¨å’Œè¡¨æ ¼
- åŒ…å« FAQ éƒ¨åˆ†

ç°åœ¨è¯·å†™å‡ºè¿™ç¯‡æ–‡ç« ã€‚å­—æ•° {int(word_count * 0.9)} - {int(word_count * 1.1)} ä¹‹é—´ã€‚"""
        
        prompt_path = f"{self.output_dir}/writing_prompt.txt"
        with open(prompt_path, 'w', encoding='utf-8') as f:
            f.write(prompt)
        
        self._log("âœ… å†™ä½œæç¤ºå·²ç”Ÿæˆ\\n")
        self._log("ã€å¤åˆ¶ä¸‹é¢å†…å®¹åˆ° ChatGPT æˆ– Claudeã€‘\\n")
        self._log("="*80)
        self._log(prompt)
        self._log("="*80 + "\\n")
        
        return prompt
    
    def step5_seo_titles(self):
        self._log(f"\\n{'='*80}")
        self._log("ã€æ­¥éª¤ 5ã€‘ç”Ÿæˆ SEO ä¼˜åŒ–çš„æ ‡é¢˜å’Œæè¿°")
        self._log(f"{'='*80}\\n")
        
        keyword_title = self.keyword.title()
        
        titles = [
            f"The Ultimate {keyword_title} Guide: Complete Step-by-Step [2024]",
            f"How to {keyword_title}: Expert Strategies & Best Practices",
            f"What is {keyword_title}? Complete Beginner's Guide",
            f"{keyword_title} 101: Everything You Need to Know",
            f"Best {keyword_title} Tips: Proven Strategies from Experts",
        ]
        
        descriptions = [
            f"Learn {keyword_title} with our comprehensive guide. Discover strategies, best practices, examples, and expert tips.",
            f"Complete guide to {keyword_title}. Get step-by-step instructions, proven tactics, and professional insights.",
            f"Master {keyword_title} with our resource. Includes tips, tools, case studies, and everything you need.",
            f"Everything about {keyword_title} here. Guide, strategies, examples, and actionable advice.",
        ]
        
        self._log("ğŸ“‹ æ¨è Page Title\\n")
        for i, title in enumerate(titles, 1):
            self._log(f"{i}. ({len(title)} å­—) {title}\\n")
        
        self._log("ğŸ“ æ¨è Meta Description\\n")
        for i, desc in enumerate(descriptions, 1):
            self._log(f"{i}. ({len(desc)} å­—) {desc}\\n")
        
        with open(f"{self.output_dir}/seo_titles.txt", 'w', encoding='utf-8') as f:
            f.write("Page Titles\\n\\n")
            for i, title in enumerate(titles, 1):
                f.write(f"{i}. {title}\\n\\n")
            f.write("\\nMeta Descriptions\\n\\n")
            for i, desc in enumerate(descriptions, 1):
                f.write(f"{i}. {desc}\\n\\n")
    
    def run_all(self):
        self.step1_google_search_and_extract()
        word_count = self.step2_word_count_analysis()
        outline = self.step3_outline_analysis()
        self.step4_ai_writing_prompt(word_count, outline)
        self.step5_seo_titles()
        
        self._log(f"\\n{'='*80}")
        self._log("âœ… æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
        self._log(f"{'='*80}")
        self._log(f"\\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨ï¼š{self.output_dir}/\\n")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        keyword = input("è¯·è¾“å…¥å…³é”®è¯ï¼š")
    else:
        keyword = " ".join(sys.argv[1:])
    
    automation = SEOArticleAutomation(keyword)
    automation.run_all()
